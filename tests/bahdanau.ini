;; Small training test

[main]
name="translation bahdanau style"
tf_manager=<tf_manager>
output="tests/outputs/bahdanau"
overwrite_output_dir=True
batch_size=16
epochs=2
train_dataset=<train_data>
val_dataset=[<val_data>,<val_data>]
trainer=<trainer>
runners=[<runner>, <representation_runner>, <debug_runner>]
postprocess=None
evaluation=[("target", evaluators.ROUGE_L), ("target", evaluators.BLEU)]
logging_period=20
validation_period=60

test_datasets=[<val_data_no_target>]

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=4
num_sessions=1

[train_data]
; This is a definition of the training data object. Dataset is not a standard
; class, it treats the __init__ method's arguments as a dictionary, therefore
; the data series names can be any string, prefixed with "s_". To specify the
; output file for a series, use "s_" prefix and "_out" suffix, e.g.
; "s_target_out"
class=dataset.load_dataset_from_files
s_source="tests/data/train.tc.en"
s_target="tests/data/train.tc.de"

[val_data]
; Validation data, the languages are not necessary here, encoders and decoders
; access the data series via the string identifiers defined here.
class=dataset.load_dataset_from_files
s_source="tests/data/val.tc.en"
s_target="tests/data/val.tc.de"


[val_data_no_target]
; Validation data, the languages are not necessary here, encoders and decoders
; access the data series via the string identifiers defined here.
class=dataset.load_dataset_from_files
s_source="tests/data/val.tc.en"
s_encoded_out="tests/outputs/bahdanau/encoded"
s_debugtensors_out="tests/outputs/bahdanau/debugtensors"

[encoder_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source"]
max_size=60
save_file="tests/outputs/bahdanau/encoder_vocabulary.pickle"
overwrite=True

[encoder]
class=encoders.recurrent.SentenceEncoder
name="sentence_encoder"
rnn_size=7
max_input_len=10
embedding_size=11
data_id="source"
vocabulary=<encoder_vocabulary>

[attention]
class=attention.Attention
name="attention_sentence_encoder"
encoder=<encoder>

[decoder_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["target"]
max_size=70
save_file="tests/outputs/bahdanau/decoder_vocabulary.pickle"
overwrite=True

[decoder]
class=decoders.decoder.Decoder
name="bahdanau_decoder"
encoders=[<encoder>]
rnn_size=8
embedding_size=9
attentions=[<attention>]
output_projection=<dec_maxout_output>
dropout_keep_prob=0.5
data_id="target"
max_output_len=10
vocabulary=<decoder_vocabulary>
supress_unk=True

[dec_maxout_output]
class=decoders.output_projection.maxout_output
maxout_size=7

[trainer]
; This block just fills the arguments of the trainer __init__ method.
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
regularizers=[trainers.regularizers.L2]
clip_norm=1.0

[runner]
; This block is used for both validation and testing to run the model on
; a given dataset.
class=runners.GreedyRunner
output_series="target"
decoder=<decoder>

[representation_runner]
class=runners.tensor_runner.RepresentationRunner
encoder=<encoder>
output_series="encoded"

[debug_runner]
class=runners.tensor_runner.TensorRunner
toplevel_modelpart=<decoder>
toplevel_tensors=[<decoder.decoded>]
; tensors_by_name=["sentence_encoder/input_to_final_state/Tensordot/add:0"]
tensors_by_name=[]
batch_dims_by_name=[]
tensors_by_ref=[<encoder.output>,<encoder.temporal_states>,<decoder.runtime_logits>]
batch_dims_by_ref=[0, 0, 1]
output_series="debugtensors"
